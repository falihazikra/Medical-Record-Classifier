{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer,PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from nltk.tokenize import PunktSentenceTokenizer , TreebankWordTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, numpy, string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/capstone/capstone\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted xml to json\n",
    "df0 = pd.read_json('smokers_surrogate_all_data copy.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transposing to rearrange columns\n",
    "df=df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving columns names\n",
    "df.columns = ['id', 'smoking_status','descrp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure to see the number of records\n",
    "a4_dims = (8, 5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "sns.countplot(x=\"smoking_status\", data=df)\n",
    "plt.title(\"Number of records per smoking category\", fontsize=18)\n",
    "plt.ylabel('# of Occurrences', fontsize=14)\n",
    "plt.xlabel('Smoking Category', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNKNOWN           315\n",
       "NON-SMOKER         82\n",
       "PAST SMOKER        47\n",
       "CURRENT SMOKER     46\n",
       "SMOKER             12\n",
       "Name: smoking_status, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.smoking_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification -Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating records based on unknown and known smoking related text status\n",
    "def unknown(column):\n",
    "    if 'UNKNOWN' in column :\n",
    "        return 1\n",
    "\n",
    "    else: return 0\n",
    "\n",
    "df[\"smoking_unknown\"] = df[\"smoking_status\"].apply(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>descrp</th>\n",
       "      <th>smoking_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CURRENT SMOKER</td>\n",
       "      <td>726132880DH9749099947532747353312/7/2006 12:00...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>688127038 EH474495202045123/5/2002 12:00:00 AM...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>214735961 DH97919193790395496149/9/2000 12:00:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>070203832 DH844654367833868044754/2/2003 12:00...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109</td>\n",
       "      <td>CURRENT SMOKER</td>\n",
       "      <td>782836641 DH9369592011119747713/15/2002 12:00:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  smoking_status                                             descrp  \\\n",
       "0    1  CURRENT SMOKER  726132880DH9749099947532747353312/7/2006 12:00...   \n",
       "1   10         UNKNOWN  688127038 EH474495202045123/5/2002 12:00:00 AM...   \n",
       "2  100         UNKNOWN  214735961 DH97919193790395496149/9/2000 12:00:...   \n",
       "3  101         UNKNOWN  070203832 DH844654367833868044754/2/2003 12:00...   \n",
       "4  109  CURRENT SMOKER  782836641 DH9369592011119747713/15/2002 12:00:...   \n",
       "\n",
       "   smoking_unknown  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    315\n",
       "0    187\n",
       "Name: smoking_unknown, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.smoking_unknown.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=['summaryunsigneddisreport','yregistration','amed','date','patient','mm', 'st','amdischarge','doctor', 'hospital',\n",
    "    'surgery','pain','problem','discharge','admission','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "    'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "    'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', \n",
    "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am','be',\n",
    "    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'of', 'at', 'by', 'for', 'about', 'against', \n",
    "    'between', 'into', 'through', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "    'under', 'further', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'more', \n",
    "    'most', 'other', 'some', 'such', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'just', 'don', \"don't\",\n",
    "    'should', \"should've\", 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "    'ma', 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "    'aboard','about','above','across','along','an','and','another','any','around','as','at','below','behind','below'\n",
    "    ,'beneath','beside','beyond','certain','down','during','each','following','for','from','inside','into','its',\n",
    "    'like','minus','my','near','next','opposite','outside','out','over','plus','round','so','some','than','through',\n",
    "    'toward','underneath','unlike','yet','under','unsigneded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(description):\n",
    "    \n",
    "    \n",
    "        \n",
    "    nopunc=[char for char in description if char not in string.punctuation]\n",
    "    \n",
    "    nopunc=''.join(nopunc)\n",
    "    \n",
    "    splitnum  = re.split('(\\d+)',nopunc)\n",
    "    splitnum=' '.join(splitnum)\n",
    "    new=re.sub(\" \\d+\", \" \", splitnum)\n",
    " \n",
    "    \n",
    "    \n",
    "    swtext= [word.lower() for word in new.split() if word not in [x.upper() for x in sw]]\n",
    "    \n",
    "    return ' '.join(swtext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = df['descrp']\n",
    "y = df['smoking_unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  X_test,  y_train, y_test  =  train_test_split(X,y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(preprocessor=text_process)\n",
    "tfidf_vect.fit(X_train)\n",
    "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
    "xtest_tfidf =  tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5952380952380952\n",
      "5 0.6349206349206349\n",
      "10 0.7380952380952381\n",
      "15 0.7698412698412699\n",
      "25 0.7936507936507936\n",
      "50 0.8095238095238095\n",
      "100 0.8333333333333334\n",
      "150 0.8492063492063492\n",
      "200 0.8333333333333334\n",
      "250 0.8412698412698413\n",
      "300 0.8571428571428571\n",
      "500 0.8492063492063492\n",
      "700 0.8650793650793651\n",
      "1000 0.8650793650793651\n",
      "1100 0.8571428571428571\n",
      "1200 0.8571428571428571\n",
      "1300 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "treelist=[1,5,10,15,25,50,100,150,200,250,300,500,700,1000,1100,1200,1300]\n",
    "for trees in treelist:\n",
    "# for trees in range(25,300,25):\n",
    "    regr = RandomForestClassifier(n_estimators=trees,random_state=42)\n",
    "    regr.fit(xtrain_tfidf, y_train)\n",
    "    print(trees, regr.score(xtest_tfidf,y_test))\n",
    "# print(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   49.7s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 25.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, 4000, 5000, 7000, 8000], 'clf__max_fe... 5, 7, 10], 'clf__min_samples_leaf': [1, 25, 50, 75, 100], 'clf__random_state': [15325, 42, 100, 1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip = Pipeline([\n",
    "    ('vect', TfidfVectorizer(sublinear_tf=True,\n",
    "                             preprocessor=text_process,\n",
    "                             )),\n",
    "     ('clf',RandomForestClassifier(n_estimators=300) )])\n",
    "\n",
    "# Number of trees in random forest\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt',0.2]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 100, num = 5)]\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "\n",
    "\n",
    "rf_param = {'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "               'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "              'clf__max_features': max_features,\n",
    "              'clf__max_depth': max_depth,\n",
    "              'clf__min_samples_split': min_samples_split,\n",
    "              'clf__min_samples_leaf': min_samples_leaf,\n",
    "              'clf__random_state':[15325,42,100,1]\n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = pip, param_distributions = rf_param, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': True,\n",
       " 'vect__norm': 'l1',\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__min_df': 1,\n",
       " 'vect__max_features': 7000,\n",
       " 'vect__max_df': 1.0,\n",
       " 'clf__random_state': 100,\n",
       " 'clf__min_samples_split': 7,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__max_features': 0.2,\n",
       " 'clf__max_depth': 90}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9676569864763668"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 24.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function text_process at 0x...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'tfidf__sublinear_tf': (True, False), 'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001, 1e-05)},\n",
       "          pre_dispatch='2*n_jobs', random_state=15325, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_nb = Pipeline([\n",
    "    ('vect', CountVectorizer(preprocessor=text_process)),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf',MultinomialNB() )])\n",
    "\n",
    "\n",
    "\n",
    "nb_param = {'vect__min_df':[1,2,3,4,5], \n",
    "           'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "           'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "           'tfidf__use_idf': (True, False),\n",
    "            'tfidf__norm': ('l1', 'l2'),\n",
    "           'tfidf__sublinear_tf': (True, False),\n",
    "           'clf__alpha':(1, 0.1, 0.01, 0.001, 0.0001, 0.00001)\n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "nb_random = RandomizedSearchCV(random_state=15325,estimator = pip_nb, param_distributions = nb_param, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "nb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__ngram_range': (1, 2),\n",
       " 'vect__min_df': 5,\n",
       " 'vect__max_df': 0.6,\n",
       " 'tfidf__use_idf': False,\n",
       " 'tfidf__sublinear_tf': True,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'clf__alpha': 1e-05}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.847212127219065"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.626984126984127\n",
      "5 0.873015873015873\n",
      "10 0.8809523809523809\n",
      "15 0.8888888888888888\n",
      "25 0.9126984126984127\n",
      "50 0.9365079365079365\n",
      "100 0.9444444444444444\n",
      "150 0.9444444444444444\n",
      "200 0.9444444444444444\n",
      "250 0.9365079365079365\n",
      "300 0.9444444444444444\n",
      "500 0.9365079365079365\n",
      "700 0.9365079365079365\n",
      "1000 0.9365079365079365\n",
      "1100 0.9365079365079365\n",
      "1200 0.9365079365079365\n",
      "1300 0.9365079365079365\n"
     ]
    }
   ],
   "source": [
    "treelist=[1,5,10,15,25,50,100,150,200,250,300,500,700,1000,1100,1200,1300]\n",
    "for trees in treelist:\n",
    "# for trees in range(25,300,25):\n",
    "    regr = GradientBoostingClassifier(n_estimators=trees,random_state=42)\n",
    "    regr.fit(xtrain_tfidf, y_train)\n",
    "    print(trees, regr.score(xtest_tfidf,y_test))\n",
    "# print(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 24.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__sublinear_tf': (True, False), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, ...__subsample': [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1], 'clf__max_features': ['auto', 'sqrt', 0.2, 0.1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=15325, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_gb = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                             preprocessor=text_process, \n",
    "                             stop_words=sw,\n",
    "                             )),\n",
    "     ('clf',GradientBoostingClassifier(n_estimators=100) )])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "\n",
    "\n",
    "gb_params = {'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__sublinear_tf': (True, False),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "                'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "                  'clf__learning_rate': [0.15,0.1,0.05,0.02,0.01,0.005,0.001],\n",
    "                  'clf__min_samples_split':[int(x) for x in np.linspace(2, 20, num = 2)],\n",
    "              'clf__max_depth': [int(x) for x in np.linspace(1, 15, num = 2)],\n",
    "              'clf__min_samples_leaf': [int(x) for x in np.linspace(1, 100, num = 5)],\n",
    "              'clf__subsample':[0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "                  'clf__max_features':['auto','sqrt',0.2,0.1]\n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(random_state=15325,estimator = pip_gb, param_distributions = gb_params, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98638275846879"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': True,\n",
       " 'vect__sublinear_tf': False,\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__min_df': 4,\n",
       " 'vect__max_features': 3000,\n",
       " 'vect__max_df': 0.8,\n",
       " 'clf__subsample': 0.8,\n",
       " 'clf__min_samples_split': 20,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_depth': 15,\n",
       " 'clf__learning_rate': 0.05}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 23.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__sublinear_tf': (True, False), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, 4000, 5000, 7000, 8000], 'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'clf__penalty': ['l1', 'l2']},\n",
       "          pre_dispatch='2*n_jobs', random_state=15325, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_lr = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                             preprocessor=text_process, \n",
    "                             stop_words=sw,\n",
    "                             )),\n",
    "     ('clf',LogisticRegression())])\n",
    "\n",
    "\n",
    "lr_params={'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__sublinear_tf': (True, False),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "                'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "           'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n",
    "              'clf__penalty':['l1','l2']}\n",
    "\n",
    "lr_random = RandomizedSearchCV(random_state=15325,estimator = pip_lr, param_distributions = lr_params, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "lr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9667353585745122"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': True,\n",
       " 'vect__sublinear_tf': True,\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__min_df': 4,\n",
       " 'vect__max_features': 3000,\n",
       " 'vect__max_df': 0.9,\n",
       " 'clf__penalty': 'l1',\n",
       " 'clf__C': 1000}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification- Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondf=df1[df1.smoking_unknown==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>descrp</th>\n",
       "      <th>smoking_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CURRENT SMOKER</td>\n",
       "      <td>726132880DH9749099947532747353312/7/2006 12:00...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109</td>\n",
       "      <td>CURRENT SMOKER</td>\n",
       "      <td>782836641 DH9369592011119747713/15/2002 12:00:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>111</td>\n",
       "      <td>NON-SMOKER</td>\n",
       "      <td>081039790 EH0586196722141427/23/2003 12:00:00 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>121</td>\n",
       "      <td>SMOKER</td>\n",
       "      <td>403426241 EH1248981382315612/13/2002 12:00:00 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>130</td>\n",
       "      <td>CURRENT SMOKER</td>\n",
       "      <td>897261359 EH7423428587654175/23/2003 12:00:00 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  smoking_status                                             descrp  \\\n",
       "0     1  CURRENT SMOKER  726132880DH9749099947532747353312/7/2006 12:00...   \n",
       "4   109  CURRENT SMOKER  782836641 DH9369592011119747713/15/2002 12:00:...   \n",
       "5   111      NON-SMOKER  081039790 EH0586196722141427/23/2003 12:00:00 ...   \n",
       "10  121          SMOKER  403426241 EH1248981382315612/13/2002 12:00:00 ...   \n",
       "11  130  CURRENT SMOKER  897261359 EH7423428587654175/23/2003 12:00:00 ...   \n",
       "\n",
       "    smoking_unknown  \n",
       "0                 0  \n",
       "4                 0  \n",
       "5                 0  \n",
       "10                0  \n",
       "11                0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Out of the known smoking records, classifiy them based on the non smoker status\n",
    "def smokerclass(column):\n",
    "    if 'NON-SMOKER' in column :\n",
    "        return 1\n",
    "\n",
    "    else: return 0\n",
    "\n",
    "secondf[\"smoking_nonsmoking\"] = secondf[\"smoking_status\"].apply(smokerclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>descrp</th>\n",
       "      <th>smoking_unknown</th>\n",
       "      <th>smoking_nonsmoking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CURRENT SMOKER</td>\n",
       "      <td>726132880DH9749099947532747353312/7/2006 12:00...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109</td>\n",
       "      <td>CURRENT SMOKER</td>\n",
       "      <td>782836641 DH9369592011119747713/15/2002 12:00:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>111</td>\n",
       "      <td>NON-SMOKER</td>\n",
       "      <td>081039790 EH0586196722141427/23/2003 12:00:00 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>121</td>\n",
       "      <td>SMOKER</td>\n",
       "      <td>403426241 EH1248981382315612/13/2002 12:00:00 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>130</td>\n",
       "      <td>CURRENT SMOKER</td>\n",
       "      <td>897261359 EH7423428587654175/23/2003 12:00:00 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  smoking_status                                             descrp  \\\n",
       "0     1  CURRENT SMOKER  726132880DH9749099947532747353312/7/2006 12:00...   \n",
       "4   109  CURRENT SMOKER  782836641 DH9369592011119747713/15/2002 12:00:...   \n",
       "5   111      NON-SMOKER  081039790 EH0586196722141427/23/2003 12:00:00 ...   \n",
       "10  121          SMOKER  403426241 EH1248981382315612/13/2002 12:00:00 ...   \n",
       "11  130  CURRENT SMOKER  897261359 EH7423428587654175/23/2003 12:00:00 ...   \n",
       "\n",
       "    smoking_unknown  smoking_nonsmoking  \n",
       "0                 0                   0  \n",
       "4                 0                   0  \n",
       "5                 0                   1  \n",
       "10                0                   0  \n",
       "11                0                   0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    105\n",
       "1     82\n",
       "Name: smoking_nonsmoking, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondf.smoking_nonsmoking.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = secondf['descrp']\n",
    "y = secondf['smoking_nonsmoking']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  X_test,  y_train, y_test  =  train_test_split(X,y,test_size=0.25, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
    "xtest_tfidf =  tfidf_vect.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6382978723404256\n",
      "5 0.723404255319149\n",
      "10 0.7021276595744681\n",
      "15 0.723404255319149\n",
      "25 0.7446808510638298\n",
      "50 0.7446808510638298\n",
      "60 0.723404255319149\n",
      "70 0.6808510638297872\n",
      "100 0.6808510638297872\n",
      "150 0.6808510638297872\n",
      "200 0.6595744680851063\n",
      "250 0.6595744680851063\n",
      "300 0.6382978723404256\n",
      "500 0.6170212765957447\n",
      "700 0.6170212765957447\n",
      "1000 0.6170212765957447\n",
      "1100 0.6170212765957447\n",
      "1200 0.5957446808510638\n",
      "1300 0.6170212765957447\n"
     ]
    }
   ],
   "source": [
    "treelist=[1,5,10,15,25,50,60,70,100,150,200,250,300,500,700,1000,1100,1200,1300]\n",
    "for trees in treelist:\n",
    "# for trees in range(25,300,25):\n",
    "    regr = RandomForestClassifier(n_estimators=trees,random_state=42)\n",
    "    regr.fit(xtrain_tfidf, y_train)\n",
    "    print(trees, regr.score(xtest_tfidf,y_test))\n",
    "# print(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 13.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, 4000, 5000, 7000, 8000], 'clf__n_esti..., 100, 110], 'clf__min_samples_split': [2, 5, 7, 10], 'clf__min_samples_leaf': [1, 25, 50, 75, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=15325, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip = Pipeline([\n",
    "    ('vect', TfidfVectorizer(sublinear_tf=True,\n",
    "                             preprocessor=text_process, \n",
    "                             stop_words=sw,\n",
    "                             )),\n",
    "     ('clf',RandomForestClassifier() )])\n",
    "\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt',0.2]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 100, num = 5)]\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "\n",
    "\n",
    "rf_param = {'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "               'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "                'clf__n_estimators':[int(x) for x in np.linspace(10, 1000, num = 10)],\n",
    "              'clf__max_features': max_features,\n",
    "              'clf__max_depth': max_depth,\n",
    "              'clf__min_samples_split': min_samples_split,\n",
    "              'clf__min_samples_leaf': min_samples_leaf,\n",
    "              \n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(random_state=15325,estimator = pip, param_distributions = rf_param, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': False,\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__min_df': 2,\n",
       " 'vect__max_features': 4000,\n",
       " 'vect__max_df': 1.0,\n",
       " 'clf__n_estimators': 450,\n",
       " 'clf__min_samples_split': 7,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__max_features': 0.2,\n",
       " 'clf__max_depth': 10}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8357993197278911"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...se_idf=True, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__sublinear_tf': (True, False), 'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001, 1e-05)},\n",
       "          pre_dispatch='2*n_jobs', random_state=15325, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_nb = Pipeline([\n",
    "    ('vect', TfidfVectorizer(preprocessor=text_process)),\n",
    "     ('clf',MultinomialNB() )])\n",
    "\n",
    "\n",
    "\n",
    "nb_param = {'vect__min_df':[1,2,3,4,5], \n",
    "           'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "           'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "           'vect__use_idf': (True, False),\n",
    "            'vect__norm': ('l1', 'l2'),\n",
    "           'vect__sublinear_tf': (True, False),\n",
    "           'clf__alpha':(1, 0.1, 0.01, 0.001, 0.0001, 0.00001)\n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "nb_random = RandomizedSearchCV(random_state=15325,estimator = pip_nb, param_distributions = nb_param, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "nb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7744047619047619"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': False,\n",
       " 'vect__sublinear_tf': True,\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__ngram_range': (2, 4),\n",
       " 'vect__min_df': 1,\n",
       " 'vect__max_df': 0.8,\n",
       " 'clf__alpha': 1e-05}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 12.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__sublinear_tf': (True, False), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, 4000, 5000, 7000, 8000], 'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'clf__penalty': ['l1', 'l2']},\n",
       "          pre_dispatch='2*n_jobs', random_state=15325, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_lr = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                             preprocessor=text_process, \n",
    "                             stop_words=sw,\n",
    "                             )),\n",
    "     ('clf',LogisticRegression())])\n",
    "\n",
    "\n",
    "lr_params={'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__sublinear_tf': (True, False),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "                'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "           'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n",
    "              'clf__penalty':['l1','l2']}\n",
    "\n",
    "lr_random = RandomizedSearchCV(random_state=15325,estimator = pip_lr, param_distributions = lr_params, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "lr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': True,\n",
       " 'vect__sublinear_tf': True,\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__min_df': 4,\n",
       " 'vect__max_features': 3000,\n",
       " 'vect__max_df': 0.9,\n",
       " 'clf__penalty': 'l1',\n",
       " 'clf__C': 1000}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8205357142857143"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6595744680851063\n",
      "5 0.7021276595744681\n",
      "10 0.7446808510638298\n",
      "15 0.723404255319149\n",
      "25 0.7021276595744681\n",
      "50 0.723404255319149\n",
      "100 0.723404255319149\n",
      "110 0.723404255319149\n",
      "120 0.723404255319149\n",
      "130 0.723404255319149\n",
      "150 0.7021276595744681\n",
      "200 0.6808510638297872\n",
      "250 0.723404255319149\n",
      "300 0.723404255319149\n",
      "500 0.723404255319149\n",
      "700 0.723404255319149\n",
      "1000 0.723404255319149\n",
      "1100 0.723404255319149\n",
      "1200 0.723404255319149\n",
      "1300 0.723404255319149\n"
     ]
    }
   ],
   "source": [
    "treelist=[1,5,10,15,25,50,100,110,120,130,150,200,250,300,500,700,1000,1100,1200,1300]\n",
    "for trees in treelist:\n",
    "# for trees in range(25,300,25):\n",
    "    regr = GradientBoostingClassifier(n_estimators=trees,random_state=42)\n",
    "    regr.fit(xtrain_tfidf, y_train)\n",
    "    print(trees, regr.score(xtest_tfidf,y_test))\n",
    "# print(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 12.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__sublinear_tf': (True, False), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, ...__subsample': [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1], 'clf__max_features': ['auto', 'sqrt', 0.2, 0.1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_gb = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                             preprocessor=text_process, \n",
    "                             stop_words=sw,\n",
    "                             )),\n",
    "     ('clf',GradientBoostingClassifier(n_estimators=50) )])\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "\n",
    "\n",
    "gb_params = {'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__sublinear_tf': (True, False),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "                'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "                  'clf__learning_rate': [0.15,0.1,0.05,0.02,0.01,0.005,0.001],\n",
    "                  'clf__min_samples_split':[int(x) for x in np.linspace(2, 20, num = 2)],\n",
    "              'clf__max_depth': [int(x) for x in np.linspace(1, 15, num = 2)],\n",
    "              'clf__min_samples_leaf': [int(x) for x in np.linspace(1, 100, num = 5)],\n",
    "              'clf__subsample':[0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "                  'clf__max_features':['auto','sqrt',0.2,0.1]\n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(estimator = pip_gb, param_distributions = gb_params, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461309523809525"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': False,\n",
       " 'vect__sublinear_tf': True,\n",
       " 'vect__norm': 'l1',\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__min_df': 2,\n",
       " 'vect__max_features': 4000,\n",
       " 'vect__max_df': 0.8,\n",
       " 'clf__subsample': 0.9,\n",
       " 'clf__min_samples_split': 20,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_depth': 15,\n",
       " 'clf__learning_rate': 0.1}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Final Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdf=secondf[secondf.smoking_nonsmoking==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "def smokerclass3rd(column):\n",
    "    if 'PAST SMOKER' in column :\n",
    "        return 1\n",
    "\n",
    "    else :return 0\n",
    "\n",
    "thirdf[\"smoking_past\"] = thirdf[\"smoking_status\"].apply(smokerclass3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 105 entries, 0 to 486\n",
      "Data columns (total 6 columns):\n",
      "id                    105 non-null int64\n",
      "smoking_status        105 non-null object\n",
      "descrp                105 non-null object\n",
      "smoking_unknown       105 non-null int64\n",
      "smoking_nonsmoking    105 non-null int64\n",
      "smoking_past          105 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 5.7+ KB\n"
     ]
    }
   ],
   "source": [
    "thirdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = thirdf['descrp']\n",
    "y = thirdf['smoking_past']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  X_test,  y_train, y_test  =  train_test_split(X,y,test_size=0.25, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
    "xtest_tfidf =  tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.4444444444444444\n",
      "5 0.5555555555555556\n",
      "10 0.5185185185185185\n",
      "15 0.5555555555555556\n",
      "25 0.5555555555555556\n",
      "50 0.5185185185185185\n",
      "100 0.5555555555555556\n",
      "150 0.48148148148148145\n",
      "200 0.5555555555555556\n",
      "250 0.5925925925925926\n",
      "300 0.5555555555555556\n",
      "500 0.5555555555555556\n",
      "700 0.5925925925925926\n",
      "1000 0.5925925925925926\n",
      "1100 0.6296296296296297\n",
      "1200 0.6296296296296297\n",
      "1300 0.5925925925925926\n"
     ]
    }
   ],
   "source": [
    "treelist=[1,5,10,15,25,50,100,150,200,250,300,500,700,1000,1100,1200,1300]\n",
    "for trees in treelist:\n",
    "# for trees in range(25,300,25):\n",
    "    regr = RandomForestClassifier(n_estimators=trees,random_state=42)\n",
    "    regr.fit(xtrain_tfidf, y_train)\n",
    "    print(trees, regr.score(xtest_tfidf,y_test))\n",
    "# print(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  8.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, 4000, 5000, 7000, 8000], 'clf__max_features': ['auto', 'sqrt', 0.2], 'clf__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110], 'clf__min_samples_split': [2, 5, 7, 10], 'clf__min_samples_leaf': [1, 25, 50, 75, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip = Pipeline([\n",
    "    ('vect', TfidfVectorizer(sublinear_tf=True,\n",
    "                             \n",
    "                             preprocessor=text_process, \n",
    "                             stop_words=sw,\n",
    "                             )),\n",
    "     ('clf',RandomForestClassifier(n_estimators=1200) )])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "\n",
    "\n",
    "rf_param = {'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "               'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "              'clf__max_features': max_features,\n",
    "              'clf__max_depth': max_depth,\n",
    "              'clf__min_samples_split': min_samples_split,\n",
    "              'clf__min_samples_leaf': min_samples_leaf,\n",
    "              \n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = pip, param_distributions = rf_param, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': True,\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__ngram_range': (1, 4),\n",
       " 'vect__min_df': 2,\n",
       " 'vect__max_features': 8000,\n",
       " 'vect__max_df': 0.7,\n",
       " 'clf__min_samples_split': 5,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__max_features': 0.2,\n",
       " 'clf__max_depth': 100}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7309829059829059"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...se_idf=True, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__sublinear_tf': (True, False), 'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001, 1e-05)},\n",
       "          pre_dispatch='2*n_jobs', random_state=15325, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_nb = Pipeline([\n",
    "    ('vect', TfidfVectorizer(preprocessor=text_process)),\n",
    "     ('clf',MultinomialNB() )])\n",
    "\n",
    "\n",
    "\n",
    "nb_param = {'vect__min_df':[1,2,3,4,5], \n",
    "           'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "           'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "           'vect__use_idf': (True, False),\n",
    "            'vect__norm': ('l1', 'l2'),\n",
    "           'vect__sublinear_tf': (True, False),\n",
    "           'clf__alpha':(1, 0.1, 0.01, 0.001, 0.0001, 0.00001)\n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "nb_random = RandomizedSearchCV(random_state=15325,estimator = pip_nb, param_distributions = nb_param, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "nb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7286324786324787"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': True,\n",
       " 'vect__sublinear_tf': False,\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__ngram_range': (2, 5),\n",
       " 'vect__min_df': 1,\n",
       " 'vect__max_df': 1.0,\n",
       " 'clf__alpha': 0.0001}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__sublinear_tf': (True, False), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, 4000, 5000, 7000, 8000], 'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'clf__penalty': ['l1', 'l2']},\n",
       "          pre_dispatch='2*n_jobs', random_state=15325, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_lr = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                             preprocessor=text_process, \n",
    "                             stop_words=sw,\n",
    "                             )),\n",
    "     ('clf',LogisticRegression())])\n",
    "\n",
    "\n",
    "lr_params={'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__sublinear_tf': (True, False),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "                'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "           'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n",
    "              'clf__penalty':['l1','l2']}\n",
    "\n",
    "lr_random = RandomizedSearchCV(random_state=15325,estimator = pip_lr, param_distributions = lr_params, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "lr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311965811965812"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': False,\n",
       " 'vect__sublinear_tf': True,\n",
       " 'vect__norm': 'l1',\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__min_df': 1,\n",
       " 'vect__max_features': 5000,\n",
       " 'vect__max_df': 0.7,\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__C': 1000}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5555555555555556\n",
      "5 0.5555555555555556\n",
      "10 0.5185185185185185\n",
      "15 0.5185185185185185\n",
      "25 0.5925925925925926\n",
      "50 0.5925925925925926\n",
      "100 0.5925925925925926\n",
      "150 0.5555555555555556\n",
      "200 0.5555555555555556\n",
      "250 0.5555555555555556\n",
      "300 0.5555555555555556\n",
      "500 0.5555555555555556\n",
      "700 0.5555555555555556\n",
      "1000 0.5555555555555556\n",
      "1100 0.5555555555555556\n",
      "1200 0.5555555555555556\n",
      "1300 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "treelist=[1,5,10,15,25,50,100,150,200,250,300,500,700,1000,1100,1200,1300]\n",
    "for trees in treelist:\n",
    "# for trees in range(25,300,25):\n",
    "    regr = GradientBoostingClassifier(n_estimators=trees,random_state=42)\n",
    "    regr.fit(xtrain_tfidf, y_train)\n",
    "    print(trees, regr.score(xtest_tfidf,y_test))\n",
    "# print(rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_pr...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'vect__sublinear_tf': (True, False), 'vect__min_df': [1, 2, 3, 4, 5], 'vect__max_df': [0.6, 0.7, 0.8, 0.9, 1.0], 'vect__ngram_range': [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)], 'vect__max_features': [2000, 3000, ...__subsample': [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1], 'clf__max_features': ['auto', 'sqrt', 0.2, 0.1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_gb = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                             preprocessor=text_process, \n",
    "                             stop_words=sw,\n",
    "                             )),\n",
    "     ('clf',GradientBoostingClassifier(n_estimators=100) )])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "\n",
    "\n",
    "gb_params = {'vect__use_idf': (True, False),\n",
    "                'vect__norm': ('l1', 'l2'),\n",
    "               'vect__sublinear_tf': (True, False),\n",
    "               'vect__min_df':[1,2,3,4,5], \n",
    "                'vect__max_df':[0.6,0.7,0.8,0.9,1.0],\n",
    "                'vect__ngram_range':[(1,2),(1,3),(1,4),(2,3),(2,4),(2,5)],\n",
    "               'vect__max_features':[2000,3000,4000,5000,7000,8000],\n",
    "                  'clf__learning_rate': [0.15,0.1,0.05,0.02,0.01,0.005,0.001],\n",
    "                  'clf__min_samples_split':[int(x) for x in np.linspace(2, 20, num = 2)],\n",
    "              'clf__max_depth': [int(x) for x in np.linspace(1, 15, num = 2)],\n",
    "              'clf__min_samples_leaf': [int(x) for x in np.linspace(1, 100, num = 5)],\n",
    "              'clf__subsample':[0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "                  'clf__max_features':['auto','sqrt',0.2,0.1]\n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(estimator = pip_gb, param_distributions = gb_params, cv = 10,n_iter = 100,scoring='roc_auc' ,verbose=1,  n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.719871794871795"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__use_idf': False,\n",
       " 'vect__sublinear_tf': False,\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__min_df': 4,\n",
       " 'vect__max_features': 4000,\n",
       " 'vect__max_df': 0.8,\n",
       " 'clf__subsample': 0.8,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__max_features': 'sqrt',\n",
       " 'clf__max_depth': 15,\n",
       " 'clf__learning_rate': 0.005}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
